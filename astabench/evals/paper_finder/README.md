
# Paper Finding Bench

PaperFindingBench assesses an agent’s ability to locate sets of papers based on a natural-language description, which may involve both content and metadata (such as the author or publication year).

## Format

### Input to agent

```json
{
  "input": {
    "query_id": "29",
    "query": "visual question answering papers using Earth Mover's Distance (EMD) as an evaluation metric"
  }
}

```

### Expected agent's output

The expected agent output should be a structured one set in the state.output.completion in the following format,

```json
{
  "output": {
    "query_id": "29",
    "results": [
      {
        "paper_id": "173990882",
        "markdown_evidence": "In particular, we used Earth Mover Distance (EMD) (Rubner et al., 2000) to compare the highly-attended objects in the VQA process to the objects highlighted by human judges"
      }
    ]
  }
}
```

### IMPORTANT NOTES:
1. Results should be ordered by most relevant first (order matters!).
2. We expect and encourage paper_id to be a SemanticScholar CorpusID.
3. The markdown_evidence:
   1. MUST be taken verbatim from the paper.
   2. MUST be the minimal snippet capturing the evidence. 

These requirements are also described in the [task instructions](https://github.com/allenai/asta-bench/blob/main/astabench/evals/paper_finder/task.py#L41) given to the agent.

## Detailed Dataset Description
PaperFindingBench is designed to be **challenging** (including things that our system currently does not perform well on) and **realistic** (based to the extent possible on real-world queries and information needs).

It also aims to be **broad and diverse** in two axes:
- first, it covers a broader set of information needs. Unlike existing datasets that focus on semantic queries that search for a set of unknown-to-the-user papers based on description of their content, our benchmark includes also “navigational” queries that seek a single known-to-the-user paper based on a short reference (“the alpha-geometry paper”), and queries that define paper sets based on a wide set of metadata criteria (“acl 2024 papers that cite the transformers paper”).
- The second axis of diversity is within the semantic-search category, in which we seek to include different types of query challenges.

The dataset mixes the different categories, and doesn’t clearly indicate which query belongs to which category (even though a human will very easily tell). This is following our belief that a literature-search agent should be able to handle all these query types, even if by merely routing them to different sub-agents.

PaperFindingBench includes:
- 48 navigational queries
- 43 metadata queries
- 242 semantic queries

Some of the metadata queries contain (easy) navigational queries as part of their criteria, but there is currently a strict separation between metadata and semantic queries (metadata queries do not involve a semantic component and vice-versa), which may change in future versions.

## Evaluation

Evaluation retrieval is challenging, as it ideally requires a gold-set of all relevant documents in the corpus, which is often not known.

Such a gold-set **is** available for the **navigational and the metadata queries** (each metadata query is internally associated with python code that uses the APIs to solve it completely, and whose results we use as the gold set).

For the **semantic queries**, the full-coverage gold-set does not exist, and we resort to a combination of partial annotation and LLM-based judgement. Each query is associated with a (potentially empty) small-set of known-to-be-good matches, as well as with a weighted set of relevance criteria that should be individually verified by the LLM against evidence from the paper for the paper to be considered a good match.

The individual relevance criteria were automatically generated by an LLM based on a (potentially expanded version of) the original query. For a fifth of the queries, the relevance criteria were manually verified and corrected or tweaked. As the tweaks and corrections turned out to be mostly minimal, and as the LLM-based relevance criteria were proved to be highly effective for the queries for which manual annotation for some papers is available, we consider all the relevance criteria as reliable, though they may be further improved in future versions.

As we aim to assess retrieval and not the judging-LLM’s ability to handle long-contexts, we don’t provide the paper’s full-text for relevance judgement but rather require each result item to be associated with extracted evidence text (either from the paper itself or from papers citing it), which is then fed to the LLM for relevance judgement.

## Scoring Metrics
We use two different scoring metrics.

For the **navigational and metadata queries**, for which the gold-set is known, we use F1 over the result-set to score individual queries. 

For the **semantic queries**, which are based on LLM judgement, we can compute precision, but not recall. One potential metric would be simply the number of returned documents that are LLM-judged to be relevant, however, this number is unbounded and harder to integrate with other scores in AstaBench. We thus opted to compute recall over an **estimated** set size for each query (that is, we divide by an estimated set size and not a definitive one), to bound the numbers between 0 and 1. The estimated set size is determined by running multiple variations of our internal agent (PaperFinder) with very lenient threshold, taking the union of the resulting set, and then multiplying it by a factor that ranges from 2 to 10 to estimate an upper bound and allow room for additional papers (smaller initial sets are less reliable and are multiplied by a larger number). Note that in extreme cases, this may result in a recall number larger than 1. We bound this by considering the retrieval-adjusted metric of $recall@k$ where we set $k$ to be the estimated set size (this corresponds to the established $recall@R$ metric, but we compute $estimated-recall@estimated$). Computing recall@k fulfills two purposes: it bounds the score in 1, and also discourages submission of “junk” results.

We balance recall@k not by precision, but by nDCG, as it provides a more relevant signal (favoring ranking relevant documents over irrelevant ones). The combination of nDCG and recall@estimated makes precision mostly redundant. To provide a single score for each individual query, we combine the recall and nDCG numbers using an harmonic mean (F1 over estimated-recall and nDCG).

## Unified Score

To provide a single unified score for the entire dataset, we average the individual query scores, overall queries regardless of their type.

## Test/Dev Split

The benchmark's purpose is to evaluate agents, thus we include a test set and a dev set, where the dev part is substantially smaller than the test set.
