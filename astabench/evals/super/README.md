
## SUPER task

This task is from the [SUPER benchmark](https://arxiv.org/abs/2409.07440) and evaluates an agent's ability to reproduce results from research repositories.

## Format

There are two kinds of scores for this task: An output-based metric (the primary metric for the test split), and trajectory-based metrics (primary metric for validation split, secondary/informational metrics for test split).  The format for the output-based metric is specified in the sample inputs and can be returned the same way as an other output-based task (via `state.output.completion`).

For the trajectory-based metrics, **it is necessary for the agent to use a SandboxJupyter-compatible execution tool**, which tracks the sequence of code steps taken by the agent and automatically makes them available to the scorer.  A compatible `python_session` tool is provided by the task in `state.tools`.

An alternative (for task-specific solvers with custom code-exec setups) is for the agent to return a json object `{"exec_result": ..., "history": ..., "submitted": 1}`, where `exec_result` is the output of the code execution (used for the output-based metric) and `history` is a list of code steps taken by the agent in the form:
```json
{
    "action": {
        "type": "execute",
        "content": <code>
    },
    "observation": <output of the code execution>,
    "execution_start_time": <rfc3339 timestamp>,
    "execution_end_time": <rfc3339 timestamp>,
}
```
