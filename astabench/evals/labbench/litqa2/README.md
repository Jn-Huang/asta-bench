# LitQA2 Task

This directory contains the InspectAI wrapper for LAB-bench's LitQA2 task, which presents multiple choice questions about scientific literature. Each question has a correct answer, an "Insufficient information" option, and several distractors.

There are two variants of the task defined in `task.py`: The `litqa2_inspect` task uses Inspect's multiple-choice format (which passes options in `state.choices`), while the `litqa2` task uses a plain text format (which passes the full task instructions and choices in the user input for each task sample).  The latter is the default for astabench, as it allows general-purpose solvers to work without special code for reading `state.choices`.

## Setup

From repo root, run:
```sh
pip install -e "."
```

## Task Description

- Multiple choice questions about scientific literature
- Each question has:
  - A correct answer
  - Several distractor options
  - An "Insufficient information" option (the model can use this to indicate
    uncertainty rather than guessing a wrong answer; see coverage/precision metrics)
- Questions are sourced from LAB-bench's HuggingFace dataset (`futurehouse/lab-bench`, subset `LitQA2`)
- We report the three metrics from the LAB-bench paper:
    - accuracy (fraction correct of all answers)
    - coverage (fraction of answers that are not "Insufficient information")
    - precision (fraction of correct answers among non-"Insufficient information" answers)

## Answer Format

The task supports two ways of providing answers:

1. Using InspectAI's multiple-choice format (recommended):
   - The solver should mark choices as true/false in the `state.choices` object
   - This is the recommended approach as it's more robust
   
   Example:
   ```python
   async def my_solver(state: TaskState, generate=None):
       # Your logic to determine the answer.
       # state.input_text contains the question and state.choices contains the choices.
       # This example always picks (A).
       answer_idx = 0
       
       # Mark choices with a single choice as correct
       for idx in range(len(state.choices)):
           state.choices.mark_choice(idx, idx == answer_idx)
       
       return state
   ```

2. Using LAB-bench's text format:
   - The solver outputs answers in either:
      - LAB-bench's format: `[ANSWER]A[/ANSWER]` for example, to indicate choice A
      - JSON format: e.g. `{"answer": "A"}`
   - The task uses the `astabench.evals.utils.mark_multichoice_answer()` parser

## Usage

You can run the eval using InspectAI's CLI (e.g. with the [multiple choice
solver](https://inspect.ai-safety-institute.org.uk/solvers.html#sec-multiple-choice)):

```bash
# Using LAB-bench's text format
inspect eval astabench/evals/labbench/litqa2 \
    --model openai/gpt-4o-mini \
    --solver multiple_choice \
    -S template='"Answer with [ANSWER]<letter>[/ANSWER].\n\nQuestion: {question}\n\nChoices:\n{choices}"'
```

Or programmatically with a custom solver:

```python
# Using InspectAI's multiple-choice format (recommended)
import inspect_ai
from inspect_ai.solver import Generate, TaskState, solver

@solver
def litqa2_solver():
    async def solve(state: TaskState, generate: Generate):
        # Your logic to determine the answer.
        # state.input_text contains the question and state.choices contains the choices.
        # This example always picks (A).
        answer_idx = 0

        # Mark choices with a single choice as correct
        for idx in range(len(state.choices)):
            state.choices.mark_choice(idx, idx == answer_idx)

        return state

    return solve

inspect_ai.eval(
    "astabench/evals/labbench/litqa2",
    # Model is not used in the example solver above
    model="mockllm/model",
    solver=litqa2_solver(),
)
```

## References

- [LAB-bench Repository](https://github.com/Future-House/LAB-Bench)
- [LAB-bench Paper](https://arxiv.org/abs/2407.10362)
- [LAB-bench HuggingFace Dataset](https://huggingface.co/datasets/futurehouse/lab-bench)
