# This file defines the pipeline for running all strong solvers and
# then scoring them on the dev set. We have a different stage defined
# for each solver type since the commands differ significantly. All
# scoring is done in the same stage (in a dvc foreach loop) since the
# scoring command is the same for all solvers.

vars:
  # Used for specifying details for retrieverless solvers:
  - llm_defaults: &llm_defaults
      llm_args: ""
      suffix: ""

stages:
  solve_sqa:
    matrix:
      model:
        - claude-3.7
        - gemini-2.5-pro
        - claude-4.0
      split:
        - dev
        - test
    # run the solver then rename the resulting file to have a nice name:
    cmd:
      INSPECT_EVAL_LOG_FILE_PATTERN=task_sqa_solver_sqa_${item.model}
      uv run --extra sqa
      inspect eval astabench/evals/sqa/task.py@sqa --display none
      --log-dir ${item.split}_dvc_logs/solver_outputs/
      --solver astabench/solvers/sqa/sqa.py@sqa_solver
      -T split=${item.split}
      -T scorer_model=${scorer_model}
      -S completion_model=${item.model}
      --limit=${limit}
      --retry-on-error=10
      --log-shared
      --no-score;
      mv "$(ls -t ${item.split}_dvc_logs/solver_outputs/*${item.model}.eval 2>/dev/null | head -n1)" "${item.split}_dvc_logs/solver_outputs/task_sqa_solver_sqa_${item.model}.eval"
    params:
      - limit
      - scorer_model
      - sqa_solver_version
    outs:
      - ${item.split}_dvc_logs/solver_outputs/task_sqa_solver_sqa_${item.model}.eval

  solve_elicit:
    # run the solver then rename the resulting file to have a nice name:
    foreach:
      - dev
      - test
    do:
      cmd:
        INSPECT_EVAL_LOG_FILE_PATTERN=task_sqa_solver_elicit
        uv run --extra sqa
        inspect eval astabench/evals/sqa/task.py@sqa --display none
        --log-dir ${item}_dvc_logs/solver_outputs/
        --solver astabench/solvers/sqa/elicit/memorized_solver.py@elicit_solver
        -T split=${item}
        -T scorer_model=${scorer_model}
        --limit=${limit}
        --log-shared
        --no-score;
        mv "$(ls -t ${item}_dvc_logs/solver_outputs/*task_sqa_solver_elicit.eval 2>/dev/null | head -n1)" "${item}_dvc_logs/solver_outputs/task_sqa_solver_elicit.eval"
      params:
        - scorer_model
        - limit
      outs:
        - ${item}_dvc_logs/solver_outputs/task_sqa_solver_elicit.eval

  solve_storm:
    # run the solver then rename the resulting file to have a nice name:
    foreach:
      - dev
      - test
    do:
      cmd:
        INSPECT_EVAL_LOG_FILE_PATTERN=task_sqa_solver_storm
        uv run --extra storm --python 3.11
        inspect eval astabench/evals/sqa/task.py@sqa --display none
        --log-dir ${item}_dvc_logs/solver_outputs/
        --solver astabench/solvers/sqa/storm_solver.py@storm_solver
        -T split=${item}
        -T scorer_model=${scorer_model}
        --limit=${limit}
        --log-shared
        --no-score;
        mv "$(ls -t ${item}_dvc_logs/solver_outputs/*task_sqa_solver_storm.eval 2>/dev/null | head -n1)" "${item}_dvc_logs/solver_outputs/task_sqa_solver_storm.eval"
      params:
        - scorer_model
        - limit
      outs:
        - ${item}_dvc_logs/solver_outputs/task_sqa_solver_storm.eval

  solve_formatted:
    matrix:
      model:
        - name: scispace
          solver_args: ""
      split:
        - dev
        - test
    # run the solver then rename the resulting file to have a nice name:
    cmd:
      INSPECT_EVAL_LOG_FILE_PATTERN=task_sqa_solver_${item.model.name}
      uv run --extra sqa
      inspect eval astabench/evals/sqa/task.py@sqa --display none
      --log-dir dvc_logs/solver_outputs/
      --solver astabench/solvers/sqa/debug/cached_solver.py@cache_solver
      -T scorer_model=${scorer_model}
      -T split=${item.split}
      -S path=allenai/${item.model.name}_sqa_response_formatted ${item.model.solver_args}
      --limit=${limit}
      --retry-on-error=10
      --log-shared
      --no-score;
      mv "$(ls -t ${item.split}_dvc_logs/solver_outputs/*${item.model.name}.eval 2>/dev/null | head -n1)" "${item.split}_dvc_logs/solver_outputs/task_sqa_solver_${item.model.name}.eval"
    params:
      - limit
      - scorer_model
      - sqa_solver_version
    outs:
      - ${item.split}_dvc_logs/solver_outputs/task_sqa_solver_${item.model.name}.eval

  # run retrieverless solvers:
  solve_llm:
    matrix:
      model:
        # fancy yaml syntax for handling structs:
        - <<: *llm_defaults
          llm_name: openai/o4-mini
          llm_args: "--reasoning-effort high --reasoning-tokens 8192 -M responses_store=false --reasoning-history none"
        - <<: *llm_defaults
          llm_name: anthropic/claude-sonnet-4-20250514
          llm_args: "--reasoning-tokens 8192"
          suffix: "-thinking"
        - <<: *llm_defaults
          llm_name: google/gemini-2.5-pro-preview-03-25
        - <<: *llm_defaults
          llm_name: anthropic/claude-sonnet-4-20250514
        - <<: *llm_defaults
          llm_name: anthropic/claude-3-7-sonnet-20250219
        - <<: *llm_defaults
          llm_name: anthropic/claude-3-5-sonnet-20240620
      split:
        - dev
        - test
    # run the solver then rename the resulting file to have a nice name:
    cmd:
      INSPECT_EVAL_LOG_FILE_PATTERN=task_sqa_solver_${item.model.llm_name}${item.model.suffix}
      uv run --extra sqa
      inspect eval astabench/evals/sqa/task.py@sqa --display plain
      --log-dir ${item.split}_dvc_logs/solver_outputs/
      --solver astabench/solvers/sqa/formatted_llm.py@formatted_solver
      --model ${item.model.llm_name} ${item.model.llm_args}
      -T split=${item.split}
      -T scorer_model=${scorer_model}
      -T excerpt_prompt=False
      --limit=${limit}
      --retry-on-error=10
      --log-shared
      --no-score;
      [[ "${item.model.llm_name}" == */* ]] && mkdir -p ${item.split}_dvc_logs/solver_outputs/task_sqa_solver_$(dirname "${item.model.llm_name}");
      mv "$(ls -t ${item.split}_dvc_logs/solver_outputs/*${item.model.llm_name}${item.model.suffix}.eval 2>/dev/null | head -n1)" "${item.split}_dvc_logs/solver_outputs/task_sqa_solver_${item.model.llm_name}${item.model.suffix}.eval"
    params:
      - limit
      - scorer_model
      - sqa_solver_version
    outs:
      - ${item.split}_dvc_logs/solver_outputs/task_sqa_solver_${item.model.llm_name}${item.model.suffix}.eval

  solve_memorized:
    matrix:
      model:
        - name: fhouse_crow
          solver_args: "-S require_snippets=false"
        - name: fhouse_falcon
          solver_args: "-S require_snippets=false"
        - name: openai_deep_research
          solver_args: ""
      split:
        - dev
        - test
    # run the solver then rename the resulting file to have a nice name:
    cmd:
      INSPECT_EVAL_LOG_FILE_PATTERN=task_sqa_solver_${item.model.name}
      uv run --extra sqa
      inspect eval astabench/evals/sqa/task.py@sqa --display none
      --log-dir dvc_logs/solver_outputs/
      --solver astabench/solvers/sqa/general_memorized/memorized_solver.py@formatted_solver
      -T scorer_model=${scorer_model}
      -T split=${item.split}
      -S sys_name_or_path=${item.model.name} ${item.model.solver_args}
      --limit=${limit}
      --retry-on-error=10
      --log-shared
      --no-score;
      mv "$(ls -t ${item.split}_dvc_logs/solver_outputs/*${item.model.name}.eval 2>/dev/null | head -n1)" "${item.split}_dvc_logs/solver_outputs/task_sqa_solver_${item.model.name}.eval"
    params:
      - limit
      - scorer_model
      - sqa_solver_version
    outs:
      - ${item.split}_dvc_logs/solver_outputs/task_sqa_solver_${item.model.name}.eval

  score_all_solvers:
    matrix:
      model:
        # retrieverless solvers:
        - openai/o4-mini
        - anthropic/claude-sonnet-4-20250514-thinking
        - google/gemini-2.5-pro-preview-03-25
        - anthropic/claude-sonnet-4-20250514
        - anthropic/claude-3-7-sonnet-20250219
        - anthropic/claude-3-5-sonnet-20240620
        # sqa solvers:
        - sqa_claude-3.7
        - sqa_claude-4.0
        - sqa_gemini-2.5-pro
        # strong baselines:
        - elicit
        - storm
        - scispace
        - fhouse_crow
        - fhouse_falcon
        - openai_deep_research
      split:
        - test
        - dev
    # copy the solver output, then score it
    cmd:
      echo "Scoring";[[ "${item.model}" == */* ]] && mkdir -p ${item.split}_dvc_logs/scored/task_sqa_solver_${item.model};
      cp ${item.split}_dvc_logs/solver_outputs/task_sqa_solver_${item.model}.eval ${item.split}_dvc_logs/scored/task_sqa_solver_${item.model}.eval;
      uv run
      inspect score
      --overwrite
      --display none
      --scorer astabench/evals/sqa/task.py@score_all
      -S scorer_model=${scorer_model}
      ${item.split}_dvc_logs/scored/task_sqa_solver_${item.model}.eval
    params:
      - sqa_scorer_version
      - scorer_model
    deps:
      - ${item.split}_dvc_logs/solver_outputs/task_sqa_solver_${item.model}.eval
    outs:
      - ${item.split}_dvc_logs/scored/task_sqa_solver_${item.model}.eval:
          persist: true

  log_any_remaining_errors:
    matrix:
      model:
        - openai/o4-mini
        - anthropic/claude-sonnet-4-20250514-thinking
        - google/gemini-2.5-pro-preview-03-25
        - anthropic/claude-sonnet-4-20250514
        - anthropic/claude-3-7-sonnet-20250219
        - anthropic/claude-3-5-sonnet-20240620
        - sqa_claude-3.7
        - sqa_claude-4.0
        - sqa_gemini-2.5-pro
        - elicit
        - storm
        - scispace
        - fhouse_crow
        - fhouse_falcon
        - openai_deep_research
      split:
        - dev
        - test
    cmd: echo "Collecting errors";[[ "${item}" == */* ]] && mkdir -p dvc_logs/errors/task_sqa_solver_$(dirname "${item}");
      uv run scripts/log_errors.py
      ${item.split}_dvc_logs/scored/task_sqa_solver_${item.model}.eval
      ${item.split}_dvc_logs/errors/task_sqa_solver_${item.model}.json
      ${item.split}_dvc_logs/errors/task_sqa_solver_${item.model}.md
    deps:
      - ${item.split}_dvc_logs/scored/task_sqa_solver_${item.model}.eval
    outs:
      - ${item.split}_dvc_logs/errors/task_sqa_solver_${item.model}.json
      - ${item.split}_dvc_logs/errors/task_sqa_solver_${item.model}.md

  extract_model_responses:
    matrix:
      model:
        - openai/o4-mini
        - anthropic/claude-sonnet-4-20250514-thinking
        - google/gemini-2.5-pro-preview-03-25
        - anthropic/claude-sonnet-4-20250514
        - anthropic/claude-3-7-sonnet-20250219
        - anthropic/claude-3-5-sonnet-20240620
        - sqa_claude-3.7
        - sqa_claude-4.0
        - sqa_gemini-2.5-pro
        - elicit
        - storm
        - scispace
        - fhouse_crow
        - fhouse_falcon
        - openai_deep_research
      split:
        - dev
        - test
    cmd: uv run scripts/extract_model_responses.py
      ${item.split}_dvc_logs/solver_outputs/task_sqa_solver_${item.model}.eval
      ${item.split}_dvc_logs/model_responses/
    deps:
      - ${item.split}_dvc_logs/solver_outputs/task_sqa_solver_${item.model}.eval
    outs:
      - ${item.split}_dvc_logs/model_responses/task_sqa_solver_${item.model}_responses.csv

  create_nice_logs:
    matrix:
      model:
        - openai/o4-mini
        - anthropic/claude-sonnet-4-20250514-thinking
        - google/gemini-2.5-pro-preview-03-25
        - anthropic/claude-sonnet-4-20250514
        - anthropic/claude-3-7-sonnet-20250219
        - anthropic/claude-3-5-sonnet-20240620
        - sqa_claude-3.7
        - sqa_claude-4.0
        - sqa_gemini-2.5-pro
        - elicit
        - storm
        - scispace
        - fhouse_crow
        - fhouse_falcon
        - openai_deep_research
      split:
        - dev
        - test
    cmd: uv run scripts/create_debug_logs.py
      ${item.split}_dvc_logs/scored/task_sqa_solver_${item.model}.eval
      ${item.split}_dvc_logs/debug_logs/
    deps:
      - ${item.split}_dvc_logs/scored/task_sqa_solver_${item.model}.eval
    outs:
      - ${item.split}_dvc_logs/debug_logs/task_sqa_solver_${item.model}_rubric_eval.csv
      - ${item.split}_dvc_logs/debug_logs/task_sqa_solver_${item.model}_citation_eval.csv
      - ${item.split}_dvc_logs/debug_logs/task_sqa_solver_${item.model}_answer_precision_eval.csv

  summarize_scores_test:
    cmd: uv run scripts/summarize_scores.py
      test_dvc_logs/scored/task_sqa_solver_openai/o4-mini.eval
      test_dvc_logs/scored/task_sqa_solver_anthropic/claude-sonnet-4-20250514-thinking.eval
      test_dvc_logs/scored/task_sqa_solver_google/gemini-2.5-pro-preview-03-25.eval
      test_dvc_logs/scored/task_sqa_solver_anthropic/claude-sonnet-4-20250514.eval
      test_dvc_logs/scored/task_sqa_solver_anthropic/claude-3-7-sonnet-20250219.eval
      test_dvc_logs/scored/task_sqa_solver_anthropic/claude-3-5-sonnet-20240620.eval
      test_dvc_logs/scored/task_sqa_solver_sqa_claude-3.7.eval
      test_dvc_logs/scored/task_sqa_solver_sqa_claude-4.0.eval
      test_dvc_logs/scored/task_sqa_solver_sqa_gemini-2.5-pro.eval
      test_dvc_logs/scored/task_sqa_solver_elicit.eval
      test_dvc_logs/scored/task_sqa_solver_storm.eval
      test_dvc_logs/scored/task_sqa_solver_fhouse_crow.eval
      test_dvc_logs/scored/task_sqa_solver_fhouse_falcon.eval
      test_dvc_logs/scored/task_sqa_solver_openai_deep_research.eval
      -o test_dvc_logs/eval_comparison.md
    deps:
      - test_dvc_logs/scored/task_sqa_solver_openai/o4-mini.eval
      - test_dvc_logs/scored/task_sqa_solver_anthropic/claude-sonnet-4-20250514-thinking.eval
      - test_dvc_logs/scored/task_sqa_solver_google/gemini-2.5-pro-preview-03-25.eval
      - test_dvc_logs/scored/task_sqa_solver_anthropic/claude-sonnet-4-20250514.eval
      - test_dvc_logs/scored/task_sqa_solver_anthropic/claude-3-7-sonnet-20250219.eval
      - test_dvc_logs/scored/task_sqa_solver_anthropic/claude-3-5-sonnet-20240620.eval
      - test_dvc_logs/scored/task_sqa_solver_sqa_claude-3.7.eval
      - test_dvc_logs/scored/task_sqa_solver_sqa_claude-4.0.eval
      - test_dvc_logs/scored/task_sqa_solver_sqa_gemini-2.5-pro.eval
      - test_dvc_logs/scored/task_sqa_solver_elicit.eval
      - test_dvc_logs/scored/task_sqa_solver_storm.eval
      - test_dvc_logs/scored/task_sqa_solver_fhouse_crow.eval
      - test_dvc_logs/scored/task_sqa_solver_fhouse_falcon.eval
      - test_dvc_logs/scored/task_sqa_solver_openai_deep_research.eval
    outs:
      - test_dvc_logs/eval_comparison.md:
          cache: false

  summarize_scores_dev:
    cmd: uv run scripts/summarize_scores.py
      dev_dvc_logs/scored/task_sqa_solver_openai/o4-mini.eval
      dev_dvc_logs/scored/task_sqa_solver_anthropic/claude-sonnet-4-20250514-thinking.eval
      dev_dvc_logs/scored/task_sqa_solver_google/gemini-2.5-pro-preview-03-25.eval
      dev_dvc_logs/scored/task_sqa_solver_anthropic/claude-sonnet-4-20250514.eval
      dev_dvc_logs/scored/task_sqa_solver_anthropic/claude-3-7-sonnet-20250219.eval
      dev_dvc_logs/scored/task_sqa_solver_anthropic/claude-3-5-sonnet-20240620.eval
      dev_dvc_logs/scored/task_sqa_solver_sqa_claude-3.7.eval
      dev_dvc_logs/scored/task_sqa_solver_sqa_claude-4.0.eval
      dev_dvc_logs/scored/task_sqa_solver_sqa_gemini-2.5-pro.eval
      dev_dvc_logs/scored/task_sqa_solver_elicit.eval
      dev_dvc_logs/scored/task_sqa_solver_storm.eval
      -o dev_dvc_logs/eval_comparison.md
    deps:
      - dev_dvc_logs/scored/task_sqa_solver_openai/o4-mini.eval
      - dev_dvc_logs/scored/task_sqa_solver_anthropic/claude-sonnet-4-20250514-thinking.eval
      - dev_dvc_logs/scored/task_sqa_solver_google/gemini-2.5-pro-preview-03-25.eval
      - dev_dvc_logs/scored/task_sqa_solver_anthropic/claude-sonnet-4-20250514.eval
      - dev_dvc_logs/scored/task_sqa_solver_anthropic/claude-3-7-sonnet-20250219.eval
      - dev_dvc_logs/scored/task_sqa_solver_anthropic/claude-3-5-sonnet-20240620.eval
      - dev_dvc_logs/scored/task_sqa_solver_sqa_claude-3.7.eval
      - dev_dvc_logs/scored/task_sqa_solver_sqa_claude-4.0.eval
      - dev_dvc_logs/scored/task_sqa_solver_sqa_gemini-2.5-pro.eval
      - dev_dvc_logs/scored/task_sqa_solver_elicit.eval
      - dev_dvc_logs/scored/task_sqa_solver_storm.eval
    outs:
      - dev_dvc_logs/eval_comparison.md:
          cache: false
