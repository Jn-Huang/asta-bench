import pandas as pd
import inspect_ai.log as ialog
from astabench.helpers.prompt_logs import PromptLogs, build_df_from_prompt_logs
import re
from astabench.evals.sqa.task import format_report
import json
import sys
from pathlib import Path


def clean_citation(c):
    pattern = r"\((.*?)\)"
    match = re.search(pattern, c)
    if match:
        return match.group(1)
    else:
        return c


def build_df_for_logs(paths):
    dfs = []
    for p in paths:
        l = ialog.read_eval_log(p)
        for s in l.samples:
            for i, score_name in enumerate(
                ["score_sqa", "score_precision", "score_citation"]
            ):
                if s.scores["score_all"].metadata["score_meta"][i] is None:
                    continue
                df = build_df_from_prompt_logs(
                    PromptLogs(**s.scores["score_all"].metadata["score_meta"][i])
                )
                if df is None:
                    print("No prompt logs for score component:", score_name)
                    df = pd.DataFrame()
                df["eval_component"] = score_name
                df["question"] = s.metadata["initial_prompt"]
                df["case_id"] = s.metadata["case_id"]
                df["solver"] = l.plan.steps[0].solver
                df["log_file"] = p
                if score_name == "score_sqa":
                    df["score"] = s.scores["score_all"].value["ingredient_recall"]
                elif score_name == "score_precision":
                    df["score"] = s.scores["score_all"].value["answer_precision"]
                if score_name == "score_citation":
                    df["citation_recall_score"] = s.scores["score_all"].value[
                        "citation_recall"
                    ]
                    df["citation_precision_score"] = s.scores["score_all"].value[
                        "citation_precision"
                    ]
                answer = json.loads(s.output.completion)["response"]
                df["answer"] = format_report(answer)
                df["response"] = s.output.completion
                dfs.append(df)
    df = pd.concat(dfs) if dfs else pd.DataFrame()
    df["step_name"] = df.step_name.fillna("score_property")
    return df


def main():
    if len(sys.argv) < 4:
        path = sys.argv[1]
        out_dir = sys.argv[2]
        base_name = out_dir + ".".join(path.split("/")[-1].split(".")[:-1])
    else:
        log_dir = sys.argv[1]
        log_file = sys.argv[2]
        out_dir = sys.argv[3]
        path = log_dir + "/" + log_file
        base_name = f"{out_dir}/{log_file.rsplit('.', maxsplit=1)[0]}"
    filepath = Path(base_name)
    filepath.parent.mkdir(parents=True, exist_ok=True)
    df = build_df_for_logs([path])
    # The csvs contain logs for the 3 different evaluation components

    ## columns relevant to all evaluation components:
    # 'question': the question to be answered
    # 'eval_component': the evaluation component this row corresponds too
    # 'response': the output from the model
    # 'temperature': the temperature set for the judge LLM
    # 'top_p' the top_p set for the judge LLM

    ## columns relevant to rubric ingredients evaluation:
    # 'criteria': the criteria being assessedd
    # 'system_prompt': the system prompt used for the judge LLM
    # 'user_prompt': the user prompt used for the judge LLM
    # 'score`: the overall score for this sample
    # 'scores': the scores for each criteria generated by the judge LLM

    ## columns relevant to citation evaluation:
    # 'prompt': the prompt used for the judge LLM
    # 'text': the text whose use of citations is being evaluated
    # 'citations': the citations used in the text
    # 'claims': the judge-LLM generated assessment of the use of citations
    # 'citation_recall_score': the overall recall score for this sample
    # 'citation_precision_score': the overall precision score for this sample

    ## columns relevant to answer precision evaluation:
    # 'prompt': the prompt used for the judge LLM
    # 'irrelevant_texts': the texts identified as irrelevant by thejudge LLM
    # 'answer_paragraphs': the paragraphs in the answer from the model
    # 'matching_sentence_paragraph_pairs': list of pairs matching irrelevant sentences to paragraphs from the model's answer
    # 'score`: the overall score for this sample

    columns = [
        "question",
        "eval_component",
        "response",
        "answer",
        "temperature",
        "top_p",
        "criteria",
        "system_prompt",
        "user_prompt",
        "scores",
        "prompt",
        "text",
        "citations",
        "claims",
        "irrelevant_texts",
        "answer_paragraphs",
        "matching_sentence_paragraph_pairs",
    ]
    # rubric eval:
    df[df.eval_component == "score_sqa"][
        [
            "question",
            "eval_component",
            "response",
            "answer",
            "score",
            "temperature",
            "top_p",
            "criteria",
            "system_prompt",
            "user_prompt",
            "scores",
        ]
    ].to_csv(base_name + "_rubric_eval.csv", index=False)
    # citation eval:
    df[df.eval_component == "score_citation"][
        [
            "question",
            "eval_component",
            "response",
            "answer",
            "citation_recall_score",
            "citation_precision_score",
            "temperature",
            "top_p",
            "prompt",
            "text",
            "citations",
            "claims",
            "half_credit",
        ]
    ].to_csv(base_name + "_citation_eval.csv", index=False)
    # answer precision eval:
    df[df.eval_component == "score_precision"][
        [
            "question",
            "eval_component",
            "response",
            "answer",
            "score",
            "temperature",
            "top_p",
            "prompt",
            "irrelevant_texts",
            "answer_paragraphs",
            "matching_sentence_paragraph_pairs",
        ]
    ].to_csv(base_name + "_answer_precision_eval.csv", index=False)


if __name__ == "__main__":
    main()
